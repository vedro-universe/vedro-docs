---
id: debug-prompt
---

import TerminalOutput from '@site/src/components/TerminalOutput';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Debug Prompt

Auto‚Äëgenerates **AI‚Äëfriendly debug prompts** for failed Vedro scenarios, helping you quickly spot root causes and apply the *smallest* possible fix.

## ‚ú® Features

- **Zero‚Äëconfig.** Enable the plugin and you‚Äôre done.
- **Automatic prompt generation.** On every failed scenario a Markdown file is produced with steps, source, error message & traceback.
- **LLM‚Äëready.** A built‚Äëin *system prompt* instructs [ChatGPT](https://chatgpt.com) (or any LLM) to analyse, locate the bug and suggest a minimal patch.

## üõ† Installation

<Tabs>
  <TabItem value="quick" label="Quick" default>

For a one‚Äëliner install via Vedro‚Äôs plugin manager:

```shell
$ vedro plugin install vedro-debug-prompt
```

  </TabItem>
  <TabItem value="manual" label="Manual">

Prefer manual steps? No problem:

1. Install the package:

```shell
$ pip install vedro-debug-prompt
```

2. Enable the plugin in `vedro.cfg.py`:

```python
import vedro
import vedro_debug_prompt

class Config(vedro.Config):

    class Plugins(vedro.Config.Plugins):

        class DebugPrompt(vedro_debug_prompt.DebugPrompt):
            enabled = True
```

  </TabItem>
</Tabs>

## üöÄ Usage

Run your tests as usual:

```sh
$ vedro run
```

Example output when a scenario fails:

<TerminalOutput>
{`
Scenarios
[1m* [0m[1m
[0m [31m‚úó decode base64 encoded string[0m[31m
[0m   [38;5;244m|> AI Debug Prompt: .vedro/tmp/prompt_liywiyo1.md[0m[38;5;244m
[0m   [32m‚úî given_encoded_string[0m[32m
[0m   [32m‚úî when_user_decodes_string[0m[32m
[0m   [31m‚úó then_it_should_return_decoded_string[0m[31m
[0m[31m‚ï≠‚îÄ[0m[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[0m[31m [0m[1;31mTraceback [0m[1;2;31m(most recent call last)[0m[31m [0m[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ[0m[31m‚îÄ‚ïÆ[0m
[31m‚îÇ[0m [2;33m/scenarios/[0m[1;33mdecode_base64_str.py[0m:[94m20[0m in [92mthen_it_should_return[0m                  [31m‚îÇ[0m
[31m‚îÇ[0m                                                                              [31m‚îÇ[0m
[31m‚îÇ[0m   [2m17 [0m        [96mself[0m.result = b64decode([96mself[0m.encoded)                           [31m‚îÇ[0m
[31m‚îÇ[0m   [2m18 [0m                                                                        [31m‚îÇ[0m
[31m‚îÇ[0m   [2m19 [0m    [94mdef[0m[90m [0m[92mthen_it_should_return_decoded_string[0m([96mself[0m):                     [31m‚îÇ[0m
[31m‚îÇ[0m [31m‚ù± [0m20         [1;4;94massert[0m[1;4m [0m[1;4;96mself[0m[1;4m.result == [0m[1;4;33m"[0m[1;4;33mbanana[0m[1;4;33m"[0m                                  [31m‚îÇ[0m
[31m‚îÇ[0m   [2m21 [0m                                                                        [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
[1;91mAssertionError[0m
[1m>>> assert [0m[1;31mactual[0m[1m == [0m[1;32mexpected[0m
    [32m- 'banana'[0m 
    [31m+ [0m[30;41mb[0m[31m'banana'[0m 
 
 
[38;5;249m# --seed 280dc986-e618-4102-aa53-056c2876c00e[0m[38;5;249m
[0m[1;31m# 1 scenario, 0 passed, 1 failed, 0 skipped[0m[34m (0.00s)[0m[34m
[0m
`}
</TerminalOutput>


Open the referenced `prompt_*.md` file and paste its contents into ChatGPT.

:::tip
In most terminals you can ‚åò‚Äëclick (or Ctrl‚Äëclick on Linux/Windows) the file path to open it instantly
:::

A typical response might look like:

>1. **Root cause**
>   The call to `b64decode` returns a `bytes` object (`b'banana'`), but the test asserts against a Python `str` (`"banana"`), so the types (and values) don‚Äôt match.
>
>2. **Suggested fix (code)**  
>   Apply this minimal patch in `scenarios/decode_base64_str.py`:
>
>   ```diff
>    def when_user_decodes_string(self):
>   -    self.result = b64decode(self.encoded)
>   +    self.result = b64decode(self.encoded).decode('utf-8')
>   ```
>
>3. **Why this works**  
>   By calling `.decode('utf-8')` on the `bytes` result, you convert it to a `str` (`"banana"`), which matches the test‚Äôs expected value.

## ‚öôÔ∏è Custom Prompts

Need a different tone, extra context, or even a completely new set of instructions for the LLM? You can swap out the built‚Äëin `PromptBuilder` for your own implementation and override *just* the parts you care about. The most common tweak is changing the **system prompt**.

### Example: overriding the system prompt

```python
# vedro.cfg.py
import vedro
import vedro_debug_prompt


class MyPromptBuilder(vedro_debug_prompt.PromptBuilder):
    def _get_system_prompt(self) -> str:
        # Tell the LLM exactly how you want it to behave
        return "<system prompt>"


class Config(vedro.Config):
    class Plugins(vedro.Config.Plugins):
        class DebugPrompt(vedro_debug_prompt.DebugPrompt):
            enabled = True
            prompt_builder = MyPromptBuilder()  # ‚Üê use the customised builder
```

That‚Äôs it! The DebugPrompt plugin will now use your custom system prompt whenever it generates a prompt.
